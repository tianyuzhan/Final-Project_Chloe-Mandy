---
title: "Kmeans_part one"
author: "Tianyu Zhan"
date: "November 3, 2015"
output: html_document
---
  
# K-means  
Use k-means method to cluster the wines into 3 groups.
```{r,echo=F}
data(wine,package="rattle")
fit=kmeans(wine[-1],3)
```

```{r}
# the default measure to calculate k-means is Euclidean distance
library('fpc')
plotcluster(wine[-1],fit$cluster)
```  

From the graph we can see that the boundaries are not very clear, which indicates that some observations can be classified into either groups. 
Draw parallel coorinates plot to see how variables contributed in each cluster.  
```{r}
library(MASS)
parcoord(wine[-1],fit$cluster)
```  
We cannot observe a clear pattern from this graph.

Take a look at the misclassification rate (number of misclassified obs divided by total number of obs).
```{r,echo=F}
mis1=table(wine$Type,fit$cluster)
mis1
```   
There are 145(=46+20+29+50) out of 178 obs that have been misclassified, so the misclassification rate is 0.81.

To take a further look at how well the kmeans algorithm did, we use ARI (Adjusted Rand Index) as an evaluation criteria. 
$$ARI=\frac{\Sigma_{ij}\binom{n_{ij}}{2}-[\Sigma_{i}\binom{a_i}{2}\Sigma_{j}\binom{b_j}{2}]/\binom{n}{2}}{\frac{1}{2}[\Sigma_{i}\binom{a_i}{2}+\Sigma_{j}\binom{b_j}{2}]-[\Sigma_{i}\binom{a_i}{2}\Sigma_{j}\binom{b_j}{2}]/\binom{n}{2}}$$
where $n_{ij}$ denotes for the ith row and jth column in a contigency table; $a_i$ denotes for the sum of ith row and $b_j$ denotes for the sum of jth column.
```{r,echo=F}
library(flexclust)
randIndex(mis1)
```   
A small ARI indicates a poor classification.

We also want to use another internal index __Silhouette__ to evaluate the clustering result. Silhouette s(x):
$$s\left(x\right)=\frac{b\left(x\right)-a\left(x\right)}{max\left\{a\left(x\right),b\left(x\right)\right\}}$$
where cohesion a(x) is the average distance of $x$ to all other vectors in the same cluster, and separation b(x) is the average distance of $x$ to the vectors in other clusters. Its values range from -1 to 1. A high silhouette value indicates that i is well-matched to its own cluster, and poorly-matched to neighboring clusters. If most points have a high silhouette value, then the clustering solution is appropriate. If many points have a low or negative silhouette value, then the clustering solution may have either too many or too few clusters.  
```{r}
library(clusterCrit)
# an open-source package to compute all the clustering indices
intIdx=intCriteria(as.matrix(wine[-1]),fit$cluster,"all")
ccfit=cclust(as.matrix(wine[-1]),3)
set.seed(526)
plot(shadow(ccfit))
```  

From the plot we can see that in each cluster, there are some observations that have small silhouette value, which indicates a not-so-good classification.

We want to improve the performance by __scaling and centering__ (scale() function can do this work) the data.
```{r}
fit2=kmeans(scale(wine[-1]),3)
plotcluster(scale(wine[-1]),fit2$cluster)
parcoord(scale(wine[-1]),fit2$cluster)
mis2=table(wine$Type,fit2$cluster)
mis2
randIndex(mis2)
```  
We can see the data is clustered very well, with no collapse between clusters.

From the graph, we may consider that black cluster contains wine with low flavanoids value, low proanthocyanins value, low hue value; or green cluster contains wine which has dilution value higher than wine in red cluster.

This time, the misclassification rate is 0.034, much better than the previous result. ARI is 0.897, also increased a lot.    
Therefore, scaling the data can improve the clustering quality.  

```{r}
intIdx2=intCriteria(as.matrix(scale(wine[-1])),fit2$cluster,"all")
ccfit2=cclust(as.matrix(scale(wine[-1])),3)
plot(shadow(ccfit2))
```  

From the plot, we can see that most observations in each cluster have a large silhouette value, which suggests that the classification is very good.

In addition to K-means, we use __Hierarchial Clustering__--another commonly used clustering method--as an alternative method. First we consider the unscaled data.
```{r}
wine_dist=dist(wine[-1])
fit_h=hclust(wine_dist)
h_cluster=cutree(fit_h,k=3)
mis3=table(wine$Type,h_cluster)
randIndex(mis3)
```

Follow the same procedures as above, we find the ARI derived from Hierarchial Clustering is 0.371, almost the same as K-means.  
Then we try on scaled data.  
```{r}
wine_dist2=dist(scale(wine[-1]))
fit_h2=hclust(wine_dist2)
h_cluster2=cutree(fit_h2,k=3)
mis4=table(wine$Type,h_cluster2)
randIndex(mis4)
```

The ARI is only 0.577, much smaller than the result (0.897) we get from K-means. Therefore, in terms of scaled data, we think K-means is better than Hierarchial Clustering, since it has a smaller misclassification rate and larger ARI.  

